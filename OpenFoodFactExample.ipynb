{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "We demo a text classification approach based on an open dataset from https://world.openfoodfacts.org/. The goal of our talk is to categorize food items with various tags, where a particular food item is defined by a product's name, a generic name as well as a brand. For classification, we use facebooks NLP model fasttext, which provides a text classification model based on word embeddings as well as character n-gram embeddings. In a first experiment, we only use a single tag and remove additional ones from each data point. In this case, the evaluation is straight forward. However, since some classes are more closely related than others, we don't want to evaluate predictions in a binary manner as one would typically do. To this end, we implement a similarity concept and a multilabel classification approach. Additionally, we present some applications of a standardized food catalog, for instance search and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fasttext import *\n",
    "%matplotlib inline\n",
    "import evaluation_metrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import scipy.spatial.distance as sd\n",
    "import sklearn.metrics.pairwise as pw\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTTEXT_HOME=\"/Users/evelyn.trautmann/repos/fasttext3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "The following classification is based on an open dataset from https://world.openfoodfacts.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_name            324820\n",
       "generic_name             83152\n",
       "brands                  269465\n",
       "categories              329303\n",
       "categories_tags         329303\n",
       "origins                  47351\n",
       "manufacturing_places     73767\n",
       "labels                  118779\n",
       "emb_codes                52104\n",
       "countries               329099\n",
       "main_category           329297\n",
       "en_tags                 329303\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/foodcategories_single_label.csv.zip\", sep = \"\\x01\", compression=\"zip\")\n",
    "df.en_tags = df.en_tags.apply(eval)\n",
    "df = df[df.en_tags.str.len()>0]\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to classify tags based on product name, generic name and brand. In the first dataset we have a single tag assigned to each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1    329303\n",
       " Name: en_tags, dtype: int64, '87 labels ')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.en_tags.str.len().value_counts(), \"%i labels \"%len(set(np.concatenate(df.en_tags.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#generate features\n",
    "feature_cols = ['product_name', 'generic_name', 'brands']\n",
    "assert(df[df.en_tags.str.len()==0].shape[0]==0)\n",
    "X = df[feature_cols].fillna(\"\").apply(lambda x: \" \".join(x), axis = 1)\n",
    "y = df.en_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess and split train and test date\n",
    "X = X.str.lower().apply(lambda x: re.sub(r'[^\\w\\s]','',x))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "dftrain = y_train.apply(lambda x: \" \".join([\"__label__\" + y\n",
    "                    for y in x])) + \" \" + X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain_emb = y_train.apply(lambda x: \" \".join(x)) + \" \" + X_train\n",
    "dftrain_emb.to_csv(\n",
    "    \"train_emb.csv\", index = False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest = y_test.apply(lambda x: \" \".join([\"__label__\" + y\n",
    "                    for y in x])) + \" \" + X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.to_csv(\n",
    "    \"train.csv\", index = False, sep=\";\")\n",
    "dftest.to_csv(\n",
    "    \"test.csv\", index = False, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification we use fasttext, which can be either executed on command line or "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_singlelabel = train_supervised(\"train.csv\",\n",
    "                lr = 0.01,\n",
    "                autotuneValidationFile = \"test.csv\",\n",
    "                autotuneDuration = 1000)\n",
    "\n",
    "model_singlelabel.save_model(\"model_singlelabel.bin\")\n",
    "# !/Users/evelyn.trautmann/repos/fasttext3/fastText/fasttext supervised \\\n",
    "#     -input \"train.csv\" -output model_singlelabel \\\n",
    "#     -autotune-validation \"test.csv\" -autotune-duration 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_singlelabel = load_model(\"model_singlelabel.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$FASTTEXT_HOME/fastText/fasttext dump model_singlelabel.bin args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$FASTTEXT_HOME/fastText/fasttext test model_singlelabel.bin test.csv 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model_singlelabel.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_emb = train_unsupervised(\"/Users/evelyn.trautmann/projects/openfood/train_emb.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_singlelabel.test(\"test.csv\", k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = X_test.to_frame()\n",
    "df_test.columns = [\"feature\"]\n",
    "\n",
    "# determine number of labels\n",
    "df_test = df_test.join(y_test)\n",
    "\n",
    "K = int(df_test.en_tags.str.len().quantile(0.75))\n",
    "K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Label Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"prediction\"] = df_test.feature.apply(lambda x: model_singlelabel.predict(x))\n",
    "\n",
    "df_test[\"label_predicted\"] = df_test.prediction.str[0].str[0].str.replace(\"__label__\",\"\")\n",
    "df_test[\"confidence\"] = df_test.prediction.str[1].str[0]\n",
    "df_test[\"truth\"] = df_test.en_tags.str[0].str.replace(\"__label__\",\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(df_test.truth, df_test.label_predicted)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# assertion no empty tags\n",
    "assert(len(df_test.en_tags[df_test.en_tags.str.len()==0])==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"cakes\"\n",
    "df_test[df_test.truth==label].label_predicted.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the model is predicting other than the ground truth but still makes reasonable class assignments. To distinguish the reasonable assignments from the actually wrong classifications we introduce similarities between classes to count them into the accurate predicftions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example\n",
    "Consider followin common products\n",
    "chicken, poultries, seafood\n",
    "Given that the first two are pretty similar and the latter very different, a possible similarity matrix S could look like\n",
    "\n",
    "\n",
    "\n",
    "|Labels | chicken | poultries |seafood|\n",
    "|---------|:------------------|:-------------------|:--------------|\n",
    "|chicken | 1 | 0.9 | 0 |\n",
    "| poultries | 0.9 | 1 | 0 |\n",
    "| seafood | 0 | 0 | 1 |\n",
    "\n",
    "\n",
    "If we want to include now similarity into calculation of precision and recall, we have the nominator containing not only\n",
    "\n",
    "#[ Truth = chicken, Predicted = chicken ]\n",
    "\n",
    "but\n",
    "\n",
    "#[ Truth = chicken, Predicted = chicken ] * 1 + #[ Truth = chicken, Predicted = poultries ] * 0.9.\n",
    "\n",
    "Hence diagonal entries of confusion matrix \n",
    "\n",
    "$$ C_{ii} = \\#[Truth=i,Predicted=i]$$\n",
    "\n",
    "become \n",
    "\n",
    "$$ C_{ii} = \\sum_j \\#[Truth=i,Predicted=j]* S_{ij} $$\n",
    "\n",
    "That means, we count in all similar predictions weighted by degree of similatity.\n",
    "Technically that means we take the diagonal entris of\n",
    "\n",
    "$$\\hat{C}=C∗S\\hat{C} = C * S \\hat{C}=C∗S$$\n",
    "\n",
    "\n",
    "Where * denotes matrix product. Denominator remains the same: Row sum in case of precision, column sum in case of recall. Here we don't weight by similar entries since for denominator only the exact number of True (resp. Predicted ) counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "truth = [\"truth\"]\n",
    "predictions = [\"label_predicted\"]\n",
    "classes = list(set(np.concatenate(df_test[predictions + truth].apply(tuple).apply(list).values)))\n",
    "\n",
    "%time confusion = evaluation_metrics.get_confusion(df_test, classes, K, truth, predictions)\n",
    "confusion.to_csv(\"confusion_single_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute word embeddings for each class and pairwise distance\n",
    "# between every two classes. Set all distances that exceed a\n",
    "# pre-defined threshold to null and compute rbf function as similarity.\n",
    "#\n",
    "class_vecs = pd.Series({cl:model_emb.get_sentence_vector(cl) for cl in classes})\n",
    "\n",
    "Dst = class_vecs.apply(lambda x: class_vecs.apply(lambda y: sd.euclidean(x,y)))\n",
    "MAGIC_NUMBER = 0.7\n",
    "mask = Dst > MAGIC_NUMBER\n",
    "Dst[mask]=np.nan\n",
    "# print similar but not identical classes \n",
    "print(Dst.apply(np.argmax)[Dst.max()>0.0])\n",
    "\n",
    "S = np.exp(-Dst**2).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_report = evaluation_metrics.get_report(confusion, S)\n",
    "df_report.dropna().sort_values(by=\"f1score\", ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics.get_summary(df_report, confusion, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "df = pd.read_csv(\"data/foodcategories_3labels.csv.zip\", sep = \"\\x01\", compression=\"zip\")\n",
    "df.en_tags = df.en_tags.apply(eval)\n",
    "df = df[df.en_tags.str.len()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.en_tags\n",
    "feature_cols = ['product_name', 'generic_name', 'brands']\n",
    "assert(df[df.en_tags.str.len()==0].shape[0]==0)\n",
    "X = df[feature_cols].fillna(\"\").apply(lambda x: \" \".join(x), axis = 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "dftrain = y_train.apply(lambda x: \" \".join([\"__label__\" + y\n",
    "                    for y in x])) + \" \" + X_train\n",
    "dftest = y_test.apply(lambda x: \" \".join([\"__label__\" + y\n",
    "                    for y in x])) + \" \" + X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.to_csv(\n",
    "    \"train_3label.csv\", index = False, sep=\";\")\n",
    "dftest.to_csv(\n",
    "    \"test3label.csv\", index = False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3label = train_supervised(\"train_3label.csv\",\n",
    "                lr = 0.01,\n",
    "                autotunePredictions = 3,\n",
    "                autotuneValidationFile = \"test3label.csv\",\n",
    "                autotuneDuration = 1000)\n",
    "\n",
    "# !/Users/evelyn.trautmann/repos/fasttext3/fastText/fasttext supervised \\\n",
    "#     -input \"train_3label.csv\" -output model_3label \\\n",
    "#     -autotune-validation \"test3label.csv\" -autotune-duration 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3label.save_model(\"model_3label.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3label.test(\"test3label.csv\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model_3label.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let \\{$T_1^{(1)}, ...,T_M^{(1)}$\\},...,\\{$T_1^{(N)}, ...,T_M^{(N)}$\\} be our ground truth, \n",
    "\\{$P_1^{(1)}, ...,P_M^{(1)}$\\},...,\\{$P_1^{(N)}, ...,P_M^{(N)}$\\} the prediction.\n",
    "\n",
    "on diagonal we count the events of truth matching prediction\n",
    "\n",
    "$\\sum_{i=1}^N |\\{T_1^{(i)}, ...,T_M^{(i)}\\}\\cap \\{P_1^{(i)}, ...,P_M^{(i)}\\}|$\n",
    "\n",
    "so for each diagonal entry we count over all data points (N) and all predictions (M) the total amount when truth is matching prediction.\n",
    "\n",
    "$C_{kk} =\\sum_{i=1}^N \\sum_{m=1}^M \\chi{1}\\{T_m^{(i)}=k,P_m^{(i)}=k\\} $\n",
    "\n",
    "in contrast on off-diagonal we count all events where truth was not covered by prediction.\n",
    "For the off-diagonal we take only the difference sets into account. If truth for data point i was for example {fish, curry, rice-dish} and prediction was {fish, curry, soup }, we only count \n",
    "$T^i$ = {rice_dish} and $P^i$ = {soup} for the off-daigonal. Formally we do the following transformation:\n",
    "\n",
    "$\\{\\tilde{T}_m^{(i)}\\}_{q=1,..M1} = \\{T_m^{(i)}\\}_{m=1,..M}\\setminus \\{P_m^{(i)}\\}_{m=1,..M}$ \n",
    "\n",
    "$\\{\\tilde{P}_m^{(i)}\\}_{p=1,..M2} = \\{P_m^{(i)}\\}_{m=1,..M}\\setminus \\{T_m^{(i)}\\}_{m=1,..M}$\n",
    "\n",
    "Eventually we count all events of the ground truth that were not captured in predictions, which results in\n",
    "\n",
    "$C_{kl} =\\sum_{i=1}^N \\sum_{m1=1}^{M1} \\sum_{m2=1}^{M2}\n",
    "\\frac{1}{M1}\\chi{1}\\{\\tilde{T}_{m1}^{(i)}=k,\\tilde{P}_{m2}^{(i)}=l\\} $\n",
    "\n",
    "Once we have computed a multi-class - multi-label confusion matrix the classification report is straight forward.\n",
    "\n",
    "Precision for class k is computed as\n",
    "\n",
    "$Pr_k=\\frac{C_{kk}}{\\sum_j C_{kj}}$\n",
    "\n",
    "and Recall\n",
    "\n",
    "$R_k=\\frac{C_{kk}}{\\sum_j C_{jk}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = X_test.to_frame()\n",
    "df_test.columns = [\"feature\"]\n",
    "\n",
    "# determine number of labels\n",
    "df_test = df_test.join(y_test)\n",
    "\n",
    "K = int(df_test.en_tags.str.len().quantile(0.75))\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"prediction\"] = df_test.feature.apply(lambda x: model_3label.predict(x, k=K))\n",
    "\n",
    "for k in range(K):\n",
    "    df_test[\"label_predicted%i\" %k] = df_test.prediction.str[0].str[k].str.replace(\"__label__\",\"\")\n",
    "    df_test[\"confidence%i\" %k] = df_test.prediction.str[1].str[k]\n",
    "    df_test[\"truth%i\" %k] = df_test.en_tags.str[k].str.replace(\"__label__\",\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = list()\n",
    "predictions = list()\n",
    "for k in range(int(K)):\n",
    "    truth.append(\"truth%i\" %k)\n",
    "    predictions.append(\"label_predicted%i\" %k)\n",
    "classes = list(set(np.concatenate(df_test[predictions + truth].apply(tuple).apply(list).values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time confusion = evaluation_metrics.get_confusion(df_test, classes, K, truth, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report = evaluation_metrics.get_report(confusion)\n",
    "df_report.dropna().sort_values(by=\"f1score\", ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluation_metrics.get_summary(df_report, confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Multilabel Classification with Near Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_vecs = pd.Series({cl:model_emb.get_sentence_vector(cl) for cl in classes})\n",
    "\n",
    "Dst = class_vecs.apply(lambda x: class_vecs.apply(lambda y: sd.euclidean(x,y)))\n",
    "\n",
    "mask = Dst > 0.5\n",
    "Dst[mask]=np.nan\n",
    "# show entries with highes distance <= 0.5 for each class\n",
    "Dst.apply(np.argmax)[Dst.max()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.exp(-Dst**2).fillna(0)\n",
    "df_report = evaluation_metrics.get_report(confusion, S)\n",
    "df_report.dropna().sort_values(by=\"f1score\", ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics.get_summary(df_report, confusion, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openfoodfact",
   "language": "python",
   "name": "openfoodfact"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
